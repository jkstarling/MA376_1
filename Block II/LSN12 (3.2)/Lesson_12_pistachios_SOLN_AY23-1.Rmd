---
title: 'Lesson 12: Pistachios (3.2)'
author: 'LTC James K. Starling'
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, message=FALSE, echo=TRUE, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}
```

#### Review
(-2) What did we talk about in the previous lesson? Can you write out a mathematical representation of the model?  

`r colorize("We talked about multi-factor experiments.", "blue")`

$y=\mu + \alpha + \beta$.

\vspace{0.25in}

(-1) What is the null and alternative hypothesis with a multi-factor experiment? 

`r colorize("The null hypothesis is that $\\alpha$ and $\\beta$ do not have an effect on the response variable. ", "blue")`

$H_0: \alpha, \beta = 0$

$H_a: \alpha \ne 0, and/or \;  \beta \ne 0$

\vspace{0.25in}



In this chapter we're going to talk about *interactions* in multi-factor designs.  

**Background**:
Pistachios imported from the Middle East are cleaned and bleached with peroxide to turn them white. Some governments have banned bleaching over concerns of health risks; others have explored the impact on the health benefits of pistachios. Researchers (Gazor & Minaei, Dry. Technol., 2005) wanted to investigate the effects of the air velocity of the fan and the drying temperature of the oven on the amount of peroxide which remains (as a percentage) on the pistachios after the bleach process. Two values of air velocity (1.5 and 2.5 mph) and two values of drying temperature (60° and 90°F) were investigated. A full factorial design was conducted such that five batches, each consisting of 24 ounces of nuts, were randomly assigned to each treatment. We would like to determine the optimal settings of temperature and air velocity to minimize the percentage of peroxide remaining.


```{r Setup, message=FALSE, echo=FALSE, warning=FALSE}
## Load package(s) and data
library(tidyverse)
library(ggResidpanel)
## Load data  
pist.dat <- read.table('https://raw.githubusercontent.com/jkstarling/MA376/main/pistachioStudySubset.txt', header = TRUE, stringsAsFactors = TRUE)

# setwd('C:/Users/james.starling/OneDrive - West Point/Teaching/MA376/JimsLessons/Block II/LSN11/')
# pist.dat <- read.table('pistachioStudySubset.txt', header = TRUE, stringsAsFactors = TRUE)

pist.dat <- pist.dat %>% 
  mutate(Temperature = as.factor(Temperature), AirVelocity = as.factor(AirVelocity))

```

(1) Create a sources of variability diagram. What were the response variables and the explanatory variable(s)? How are these variables classified?  How many treatments do we have? 

`r colorize("The response variable is the peroxide remaining (percent) with explantory variables of temperature (two levels - 60/90 deg. F.) and air velocity (two levels - 1.5 mph and 2.5 mph). We include pistachios from the middle east. Our design indicates that we use the same oven. We have four treatments (two levels x two levels).", "blue")`

\vspace{0.25in}


(2) Are any of these explanatory variables blocking variables? Why or why not. 

`r colorize("No. The variables can be controlled by the experimenters and are not inherent properties of the pistachios.", "blue")`

\vspace{0.25in}


(3) We are treating temperature and air velocity as factors and not as continuous random variables. What is a downside of this? 

`r colorize("A possible downside is that we only get values for those two factor levels (for each). If we have a lot of levels, we will also remove degrees of freedom from the error term, reducing our ability to find a significant result.", "blue")`

\vspace{0.25in}

(4) Conduct a two-variable ANOVA with a two-factor model (use effect coding). 
Calculate the percent of variation explained by each explanatory variable and the overall model. 



```{r}
pist.dat2 <- pist.dat
contrasts(pist.dat2$Temperature) <- contr.sum
contrasts(pist.dat2$AirVelocity) <- contr.sum

pist.lm <- lm(Peroxide~ Temperature + AirVelocity, data=pist.dat2)
pist.anv <- anova(pist.lm); 
pist.anv

SST <- sum(pist.anv$`Sum Sq`)
rsq.temp <- pist.anv$`Sum Sq`[1]/SST; rsq.temp
rsq.av <- pist.anv$`Sum Sq`[2]/SST; rsq.av
rsq.mod <- rsq.temp + rsq.av; rsq.mod

```



\vspace{0.5in}


(5) What is the fitted, two-variable, statistical model?   
Which effects are statistically significant? 
According to the model, which combination is the best? (lower `Peroxide` level is better)

```{r}
summary(pist.lm)
```

`r colorize("The effect of Temperature appears to be significant (at the alpha = 0.05 level), when accouting for the effects of Airvelcity. The effect of AirVelocity appears to be insignificant (at the alpha = 0.05 level), when accouting for the effects of Temperature.", "blue")`

`r colorize("The model is: ", "blue")` $y_{ij} = \mu \phantom{ }^+_- \alpha_j \phantom{ }^+_- \beta_i +\epsilon_{ij}= 3.0885 \phantom{ }^+_- 0.4755 \phantom{ }^+_- 1.135$ 


\vspace{0.25in}


(6) According to the model, which combination is the best? (lower `Peroxide` level is better)

```{r}
pred.means <- data.frame(Temperature = c(60,   60,  90, 90),
                         AirVelocity = c(1.5, 2.5, 1.5, 2.5),
                         mn = c(NA, NA, NA, NA))
pred.means[1,'mn'] <- predict(pist.lm, data.frame(Temperature='60',AirVelocity='1.5'))
pred.means[2,'mn'] <- predict(pist.lm, data.frame(Temperature='60',AirVelocity='2.5'))
pred.means[3,'mn'] <- predict(pist.lm, data.frame(Temperature='90',AirVelocity='1.5'))
pred.means[4,'mn'] <- predict(pist.lm, data.frame(Temperature='90',AirVelocity='2.5'))
pred.means
```

`r colorize("The lowest predicted valule is 1.48 with air velocity of 2.5 and 90 degrees. ", "blue")`


(7) Calculate the means for each combination of Temperature and AirVelocity using the `group_by()` and `summarise()` commands. Is this contrary to our model predictions above? 

```{r}
pist.means <- pist.dat %>% group_by(Temperature, AirVelocity) %>% 
  summarise(mn=mean(Peroxide)) %>% as.data.frame()
pist.means
```

`r colorize("Yes it is contrary. We see that the combination of 90 degrees and air velocity of 1.5 has the lowest mean peroxide.", "blue")`

\vspace{0.25in}


(8) This is evidence of a **Statistical Interaction**. An interaction means that the effect is modified by the presence of another variable. In our experiment, if the temperature is 60, then it’d be more advantageous to have an air velocity of 2.5, but if the temperature is 90, then it’d be more advantageous to have an air velocity of 1.5.

Note that we are not saying that Air Velocity and Temperature are confounders. 
If we know the air velocity do we gain any knowledge of the temperature?

`r colorize("No. If we know air velocity, we do not gain knowledge of the temparature. We just know that it changes the effects of the temperature. ", "blue")`

\vspace{0.25in}


(9) How do we update our sources of variation diagram now?  
What is our updated statistical model? 
What is our null and alternative hypotheses? 

`r colorize("We add the interaction between temperature and air velocity to the list of explantory variables.", "blue")`

`r colorize("The model is: ", "blue")` $y_{ij} = \mu ^+_- \alpha_j ^+_- \beta_i + \alpha\beta_{ij} +\epsilon_{ij}$ 

`r colorize(" $H_0:$ There is no interaction between Temperature and AirVelocity. $H_a$: There is an interaction between Temperature and AirVelocity.", "blue")`

`r colorize(" In symbols: $H_0: \alpha\beta_{ij} = 0$ for all $i,j$ combinations.  $H_a$: At least one $\alpha\beta_{ij}$.", "blue")`


\vspace{0.45in}


(10)  Use the `pist.means` dataframe and  use the pipe command and `ggplot()` + `geom_line()` + `geom_point()` + (other stuff that makes sense) to create an interaction plot.

If there were no interaction effects, what would we expect the lines to look like? 

```{r}
p <- pist.means %>% ggplot(aes(x=Temperature, y=mn, group= AirVelocity, color=AirVelocity)) + 
  geom_line() +
  geom_point() + 
  labs(title="Interaction Plot", y="mean peroxide %") 
p
```

`r colorize("If there were no interaction effects, we would expect the lines to be parallel. ", "blue")`

\vspace{0.25in}


(11) Using the previous figure, add horizontal lines accounting for the effect of the air velocity and (separately) the effects of temperature. Use `geom_hline` to plot. 
What do you notice about dashed lines compared to the means? 

```{r}
mod.mean <- coef(pist.lm)[1];
temp.adj <- coef(pist.lm)[2];
av.adj <- coef(pist.lm)[3];

p + geom_hline(yintercept = mod.mean, linetype = "dashed") + 
  geom_hline(yintercept = mod.mean - av.adj, linetype = "dashed") + 
  geom_hline(yintercept = mod.mean + av.adj, linetype = "dashed") +
  labs(title="Interaction Plot with effects of AirVelocity") 

p + geom_hline(yintercept = mod.mean, linetype = "dashed") + 
  geom_hline(yintercept = mod.mean - temp.adj, linetype = "dashed") + 
  geom_hline(yintercept = mod.mean + temp.adj, linetype = "dashed") +
  labs(title="Interaction Plot with effects of Temperature") 
```

`r colorize("Considering the Interaction Plot with effects of AV, we see that the upper dashed line intersects the middle of the red line; this represents the main velocity effects of 0.48 and -0.48. Use a similiar explantion for the blue line.  ", "blue")`

`r colorize("Considering the Interaction Plot with effects of Temp, we see that the upper dashed line intersects the line segment between an AV of 1.5 and 2.5 when temperature is equal to 60; this represents the main velocity effects of 1.13 and -1.13. Use a similiar explantion for the lower dashed line.  ", "blue")`


(12) Using the `pist.means` table, calculate the `difference in the differences`. How do you interpret this answer? 
```{r}
pist.means

# diff of diffs degrees
dd.deg <- (pist.means$mn[1] - pist.means$mn[2]) - (pist.means$mn[3] - pist.means$mn[4])
dd.deg

# diff of diffs AirVelocity
dd.av <- (pist.means$mn[1] - pist.means$mn[3]) - (pist.means$mn[2] - pist.means$mn[4])
dd.av
```

`r colorize("Since the differences in differences is not 0, we have evidence that there might be an interaction. We see that the diff in diff is the same for both variables. ", "blue")`


\vspace{0.5in}


(12) To see if this statistic is significant, we can simulate the distribution under $H_0$ ($\alpha = \beta = 0$). How rare is it to observe something greater than 3.24?

```{r, warning=FALSE, message=FALSE}
set.seed(376)
options(dplyr.summarise.inform = FALSE)

M <- 1000
stats <- rep(NA, M)
for(i in 1:M){
  pist.dat.shuff <- pist.dat
  pist.dat.shuff$temp.shuf <- sample(pist.dat.shuff$Temperature)
  pist.dat.shuff$av.shuff <- sample(pist.dat.shuff$AirVelocity)
  p.shuff <- pist.dat.shuff %>% group_by(temp.shuf, av.shuff) %>% 
    summarise(mean.perox=mean(Peroxide))
  stats[i] <- (p.shuff$mean.perox[1] - p.shuff$mean.perox[2]) - 
              (p.shuff$mean.perox[3] - p.shuff$mean.perox[4])
}
as.data.frame(stats) %>% ggplot(aes(x=stats)) + geom_histogram(bins = 50) + geom_vline(xintercept = dd.deg, color = 'red')
sum(stats > dd.deg) / M
```

`r colorize("Based on our simulation, we estimate the p-value to be around 0.016. This indicates that seeing a diferences of differences of 3.24 is relatively rare under the null hypothesis; thus we can reject $H_0$.", "blue")`


(13) Verify the numbers of the interaction effects by using a linear model. Where do the interaction effects show up here? 

```{r}
pist.int.lm <- pist.dat2 %>% lm(Peroxide~Temperature*AirVelocity, data=.)
summary(pist.int.lm)
anova(pist.int.lm)

```


`r colorize("The magnitude of the interaction effects is the coefficient for the interatcion term in the model summary. We will have to look at the effect coding in R to determine the sign of the interaction effect. ", "blue")`


\vspace{0.25in}

(14) How does the ANOVA output change from our first model `pist.lm` to the model with interactions `pist.int.lm`? What happens to the residuals (SSE, MSE)? 

```{r}
anova(pist.lm)
anova(pist.int.lm)
```


\vspace{0.25in}


(15) We can now use the post-hoc test to see which of the interactions is significant. 

```{r}
TukeyHSD(aov(Peroxide ~ Temperature * AirVelocity, data = pist.dat2))
```


\vspace{0.25in}


