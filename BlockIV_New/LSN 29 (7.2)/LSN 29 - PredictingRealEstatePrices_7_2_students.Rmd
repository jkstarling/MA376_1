---
title: "LSN 29 - Predicting Real Estate Prices (7.2)"
author: "MA376 - CDT I.M.Smrt"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width=10, fig.height=6, message = FALSE, warning = FALSE)#, fig.show = "hide")
library(tidyverse)
library(GGally)
library(cowplot)
library(ggResidpanel)
options(tinytex.verbose = TRUE)

```

### Intro

Making predictive models of real estate data is a common technique among realtors and appraisers. We have investigated real estate data at different times earlier in this book. However, in those cases we only focused on two or three variables at a time in order to illustrate key concepts related to multi-variable thinking like interactions and correlations between explanatory variables. Now, we will investigate another real estate dataset to explore practical considerations when building a statistical model with the potential for many more than two or three explanatory variables.

One of the authors (from your book) used to own a home on the south side of Holland, Michigan. He was curious about how well statistical models would align with realtor and tax assessor valuations of his home. He decided to try to predict his home’s value using readily available characteristics of homes for sale in the area.

```{r}
homes <- read.csv("https://raw.githubusercontent.com/jkstarling/MA376/main/HomesFull.csv")
# homes <-read.csv("HomesFull.csv")
homes <- homes %>% select(-ID)
glimpse(homes)
```

1) Using your notes from last section (7.1), what might we want to check for with this dataset? 


\vspace{0.25in}


2) Create summary statistics for the mean price and standard deviation for those homes that are a) missing `Lot_Size` and b) missing `Total_Rooms`. Perform a two-sample t-test. Interpret what you see. 

```{r}
lot.mis <- homes %>% filter(is.na(Lot_Size))
lot.cmp <- homes %>% filter(!is.na(Lot_Size))
bed.mis <- homes %>% filter(is.na(Total_Rooms))
bed.cmp <- homes %>% filter(!is.na(Total_Rooms))
```

```{r}
homes %>% group_by(is.na(Lot_Size)) %>% summarise(meanPrice = mean(Asking_Price), sd = sd(Asking_Price), n=n())
lot.ttest <- t.test(lot.mis$Asking_Price, lot.cmp$Asking_Price)
lot.ttest$p.value
```

```{r}
homes %>% group_by(is.na(Total_Rooms)) %>% summarise(meanPrice = mean(Asking_Price), sd = sd(Asking_Price), n=n())
room.ttest <- t.test(bed.mis$Asking_Price, bed.cmp$Asking_Price)
room.ttest$p.value
```



\vspace{0.5in}


3) Create a pairs plot of all variables (except ID). What can we conclude based on these figures? Which variables are most strongly associated with asking price? Which explanatory variables are most strongly associated with each other?


```{r}
homes %>% ggpairs(homes) 
```


\vspace{0.5in}

4) Would it be appropriate to transform Square Footage? 

```{r , echo=FALSE}
library(cowplot)
homes$lnsqft <- log(homes$Square_Feet)
p1 <- homes %>% ggplot(aes(x = Square_Feet, y = Asking_Price)) + geom_point()
p2 <- homes %>% ggplot(aes(x = lnsqft, y = Asking_Price)) + geom_point()

plot_grid(p1,p2)
```


\vspace{0.5in}

5) What are the most promising variables after the above analysis? How does **model simplicity** play into our decision? 


\vspace{0.5in}

6) What is model overfitting? How do we avoid overfitting? Is there a way to improve this method? 



\vspace{0.5in}


7) For this dataset, we randomly selected 2/3 of the data (43 homes) to serve as the discovery sample (HomesDisc). We will subsequently validate the model on the remaining 22 homes (HomesValid). 

```{r}
homes.dsc <- read.csv("https://raw.githubusercontent.com/jkstarling/MA376/main/HomesDisc.csv")
homes.val <- read.csv("https://raw.githubusercontent.com/jkstarling/MA376/main/HomesValid.csv")
# homes.dsc <- read.csv("HomesDisc.csv")
# homes.val <- read.csv("HomesValid.csv")

homes.dsc$lnsqft <- log(homes.dsc$Square_Feet)
homes.val$lnsqft <- log(homes.val$Square_Feet)
```

Creating a linear regression model with the variables identified in 5) we get the following results. Explain what you see. 

```{r, echo=FALSE}
home.lm <- homes.dsc %>% lm(Asking_Price ~ Garage + Age + Bedrooms + Stories + Baths + lnsqft, data = .)
summary(home.lm)
```


\vspace{0.5in}


8)  Because these are the only variables with small p-values, it’s tempting to conclude that ln(sqft), age, and garage are the only important variables in predicting house price. Can we remove bedrooms, stories, and baths from the model and say that’s "good enough"? What is a downside of doing this? 


\vspace{0.5in}

9) So how do we decide the correct number of variables and which variables to use? 

- Backward elimination:  

\vspace{0.2in}

- Forward entry:

\vspace{0.2in}

- Best-subsets: 

\vspace{0.2in}

10) Using the backward elimination method, use the model created in 7) and remove those variables that are not statistically significant. What is your final model? 

```{r, include=FALSE}
# remove stories
home.lm1 <- homes.dsc %>% lm(Asking_Price ~ Garage + Age + Bedrooms + Baths + lnsqft, data = .)
summary(home.lm1)
```

```{r, include=FALSE}
# remove baths
home.lm2 <- homes.dsc %>% lm(Asking_Price ~ Garage + Age + Bedrooms + lnsqft, data = .)
summary(home.lm2)
```


\vspace{0.5in}

11) Changing gears to conduct the forward entry method. First thing to do is to compare the $R^2$ and p-values for the model with only one variable. Which variable do we select? 

```{r}
myvars <- c("Bedrooms","Baths","Garage",
            "Stories","Age","lnsqft" )

for (myvar in myvars){
  mm <- paste0("Asking_Price ~ ", noquote(myvar))
  my.lm <- homes.dsc %>% lm(mm, data = .)
  my.sum <- summary(my.lm)
  print(paste(myvar, ". r-sq: ", round(my.sum$r.squared,4), ". p-val: ", round(my.sum$coefficients[-1,-1][3],4)))
}
```


\vspace{0.5in}

12) Continue the forward selection process until there are no statistically significant variables to add. 

```{r, include=FALSE}
myvars <- c("Bedrooms","Baths","Garage",
            "Stories","lnsqft" )

for (myvar in myvars){
  mm <- paste0("Asking_Price ~ Age + ", noquote(myvar))
  my.lm <- homes.dsc %>% lm(mm, data = .)
  my.sum <- summary(my.lm)
  print(paste(myvar, ". r-sq: ", round(my.sum$r.squared,4)))
  print(round(my.sum$coefficients[3,],4))
}

myvars <- c("Bedrooms","Baths","Garage",
            "Stories" )

for (myvar in myvars){
  mm <- paste0("Asking_Price ~ Age + lnsqft +", noquote(myvar))
  my.lm <- homes.dsc %>% lm(mm, data = .)
  my.sum <- summary(my.lm)
  print(paste(myvar, ". r-sq: ", round(my.sum$r.squared,4)))
  print(round(my.sum$coefficients,4))
}

home.lm3 <- homes.dsc %>% lm(Asking_Price ~ Age + lnsqft, data = .)
summary(home.lm3)
```



\vspace{0.5in}

13) Use the `homes.val` dataset to forward selection and backward selection models you created above. How well does each model perform?   We can calculate the Mean Square Prediction Error (MSPE) for each model and compare it to the naive model. Which model does the best? 

```{r}
fs.preds <- predict(home.lm3, homes.val)
bs.preds <- predict(home.lm2, homes.val)
full.preds <- predict(home.lm, homes.val)

sum((fs.preds-homes.val$Asking_Price)^2)
sum((bs.preds-homes.val$Asking_Price)^2)
sum((full.preds-homes.val$Asking_Price)^2)

#naive model (single-mean model)
naive.pred <- mean(homes.dsc$Asking_Price)
sum((naive.pred - homes.val$Asking_Price)^2)
```






\vspace{0.5in}

14) Bonus: recreate the figures on p. 535 (Figure 7.2.14)

```{r}
pred <- data.frame(preds = predict(home.lm3, homes.val))
pred$ask <- homes.val$Asking_Price
pred$res <- pred$ask - pred$preds
p1 <- pred %>% ggplot(aes(x=res)) + geom_histogram()
p2 <- pred %>% ggplot(aes(x=preds, y=res)) + geom_point()
p3 <- pred %>% ggplot(aes(x=preds, y=ask)) + geom_point()
plot_grid(p1,p2,p3, ncol = 3)
```



